{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# Keras Hyperparameter Tuning\n",
    "\n",
    "by Fabian Märki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to tune hyperparameters of a Keras model. Due to the setup it would also be possible to search for a good model design (number of layers etc.).\n",
    "\n",
    "\n",
    "## Alternatives\n",
    "- [KerasTuner](https://keras.io/keras_tuner/)\n",
    "- [AutoKeras](https://autokeras.com/) (allows for AutoML)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/2021_HS_DAS_NLP_Notebooks/blob/master/04_b_Keras_Hyperparameter_Tuning.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fhnw-nlp-utils>=0.1.3 in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.1.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.3.4)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (0.24.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.13.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.6.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.8.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (0.70.12.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.19.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (5.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.3) (2021.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (0.10.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.3) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.3) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.3) (1.5.4)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (2.26.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (3.0.12)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (4.62.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (1.15.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.3) (2021.8.28)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.3) (7.1.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.6/dist-packages (from multiprocess->fhnw-nlp-utils>=0.1.3) (0.3.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Tensorflow version: 2.5.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "!pip install 'fhnw-nlp-utils>=0.1.6,<0.2'\n",
    "\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.colab import runs_on_colab\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "#physical_devices = tf.config.list_physical_devices('GPU') \n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 2.63 s, total: 17.5 s\n",
      "Wall time: 9.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "download(\"https://drive.google.com/uc?id=19AFeVnOfX8WXU4_3rM7OFoNTWWog_sb_\", \"data/german_doctor_reviews_tokenized.parq\")\n",
    "data = load_dataframe(\"data/german_doctor_reviews_tokenized.parq\")\n",
    "\n",
    "download(\"https://drive.google.com/uc?id=1tT2dj70GLi2bJYg4j3g1MIglGXTDAugI\", \"data/german_doctor_reviews_augmented_tokenized.parq\")\n",
    "data_aug = load_dataframe(\"data/german_doctor_reviews_augmented_tokenized.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331187, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all neutral sentimens\n",
    "data = data.loc[(data[\"label\"] != \"neutral\")]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[ich, bin, franzose, und, bin, seit, ein, paar...</td>\n",
       "      <td>ich bin franzose und bin seit ein paar wochen ...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, zahn,...</td>\n",
       "      <td>[franzos, seit, paar, woch, muench, ., zahn, s...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, ., za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[hatte, akute, beschwerden, am, rücken, ., her...</td>\n",
       "      <td>hatte akute beschwerden am rücken . herr magur...</td>\n",
       "      <td>[akut, beschwerden, rücken, magura, erste, arz...</td>\n",
       "      <td>[akut, beschwerd, ruck, ., magura, erst, arzt,...</td>\n",
       "      <td>[akute, beschwerden, rücken, ., magura, erste,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  \\\n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...     2.0   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...     1.0   \n",
       "\n",
       "                                                text     label  sentiment  \\\n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...  positive          1   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative         -1   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...  positive          1   \n",
       "\n",
       "                                         token_clean  \\\n",
       "0  [ich, bin, franzose, und, bin, seit, ein, paar...   \n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "2  [hatte, akute, beschwerden, am, rücken, ., her...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  ich bin franzose und bin seit ein paar wochen ...   \n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "2  hatte akute beschwerden am rücken . herr magur...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "0  [franzose, seit, paar, wochen, muenchen, zahn,...   \n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "2  [akut, beschwerden, rücken, magura, erste, arz...   \n",
       "\n",
       "                                          token_stem  \\\n",
       "0  [franzos, seit, paar, woch, muench, ., zahn, s...   \n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "2  [akut, beschwerd, ruck, ., magura, erst, arzt,...   \n",
       "\n",
       "                               token_clean_stopwords  \n",
       "0  [franzose, seit, paar, wochen, muenchen, ., za...  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  \n",
       "2  [akute, beschwerden, rücken, ., magura, erste,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating  sentiment  \\\n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0         -1   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0         -1   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0         -1   \n",
       "\n",
       "                                                text     label  \\\n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative   \n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative   \n",
       "\n",
       "                                         token_clean  \\\n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "\n",
       "                                         token_lemma  \\\n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "\n",
       "                                          token_stem  \\\n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "\n",
       "                               token_clean_stopwords  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhnw.nlp.utils.ploting import plot_history\n",
    "\n",
    "from fhnw.nlp.utils.params import build_model_cnn\n",
    "from fhnw.nlp.utils.params import build_model_rnn\n",
    "from fhnw.nlp.utils.params import compile_model\n",
    "from fhnw.nlp.utils.params import create_label_binarizer_and_set\n",
    "from fhnw.nlp.utils.params import dataframe_to_dataset\n",
    "from fhnw.nlp.utils.params import extract_embedding_layer_and_set\n",
    "from fhnw.nlp.utils.params import extract_text_vectorization_and_set\n",
    "from fhnw.nlp.utils.params import extract_vocabulary_and_set\n",
    "from fhnw.nlp.utils.params import get_train_test_split\n",
    "from fhnw.nlp.utils.params import re_compile_model\n",
    "from fhnw.nlp.utils.params import save_model\n",
    "from fhnw.nlp.utils.params import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"verbose\": True,\n",
    "    \"shuffle\": True,\n",
    "    \"batch_size\": 128,\n",
    "    \"X_column_name\": \"text_clean\",\n",
    "    \"y_column_name\": \"label\",\n",
    "    \"embedding_type\": \"fasttext\",\n",
    "    #\"embedding_type\": \"bytepair\",\n",
    "    \"embedding_dim\": 300,\n",
    "    \"embedding_mask_zero\": True,\n",
    "    \"embedding_trainable\": False,\n",
    "    #\"embedding_input_sequence_length\": output_sequence_length if 'output_sequence_length' in locals() or 'output_sequence_length' in globals() else None,\n",
    "    \"embedding_fasttext_model\": \"cc.de.300.bin\",\n",
    "    \"embedding_word2vec_model_url\": \"https://cloud.devmount.de/d2bc5672c523b086/german.model\",\n",
    "    \"embedding_spacy_model\": \"de_core_news_md\",\n",
    "    \"embedding_tensorflow_hub_url\": \"https://tfhub.dev/google/nnlm-de-dim128-with-normalization/2\",\n",
    "    \"model_type\": \"cnn\",\n",
    "}\n",
    "\n",
    "if runs_on_colab() and params[\"embedding_type\"] == \"fasttext\":\n",
    "    # colab as problems handling such large files\n",
    "    model_name = \"cc.de.50.bin\"\n",
    "    download(\"https://drive.google.com/uc?id=1iqw8UPEEVmzQQGmI5FkRJH8B5SkZCgXG\", model_name)\n",
    "    params[\"embedding_dim\"] = 50\n",
    "    params[\"embedding_fasttext_model\"] = model_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data, data_aug])\n",
    "#all_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred classification type: binary\n",
      "Median sequence length: 58\n",
      "Percentil (0.98) cutoff sequence length: 301\n",
      "Max sequence length: 408\n",
      "Used embedding sequence length: 301\n"
     ]
    }
   ],
   "source": [
    "create_label_binarizer_and_set(params, all_data)\n",
    "extract_vocabulary_and_set(params, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 205493\n"
     ]
    }
   ],
   "source": [
    "extract_text_vectorization_and_set(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null word embeddings:  14\n",
      "Words not found in total:  14\n",
      "Words without embedding (14/14): ['öä', 'jé', '``', 'öz', \"'t\", \"''\", 'èn', 'ìn', 'äk', 'úm', 'üw', \"'s\", '', 'wã']\n",
      "CPU times: user 6.81 s, sys: 4.58 s, total: 11.4 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "extract_embedding_layer_and_set(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training/test/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264949 train examples\n",
      "66238 test examples\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = get_train_test_split(params, data)\n",
    "data_cross_validate = pd.concat([data_train, data_aug])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/scikit-optimize/scikit-optimize.git\n",
      "  Cloning https://github.com/scikit-optimize/scikit-optimize.git to /tmp/pip-req-build-0nb68njf\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting install\n",
      "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.9.dev0) (1.5.4)\n",
      "Collecting pyaml>=16.9\n",
      "  Using cached pyaml-21.8.3-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.9.dev0) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.9.dev0) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.9.dev0) (1.19.5)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize==0.9.dev0) (5.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->scikit-optimize==0.9.dev0) (2.2.0)\n",
      "Building wheels for collected packages: scikit-optimize\n",
      "  Building wheel for scikit-optimize (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-optimize: filename=scikit_optimize-0.9.dev0-py2.py3-none-any.whl size=100309 sha256=c1e41913a24d3773b2fbb8e743eea236851220ce75677a8d454f3ec65455e429\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-itq340kq/wheels/a6/18/3a/f5a8100b6f43b1c60878a02393d97b4fe52e987aec42c1001c\n",
      "Successfully built scikit-optimize\n",
      "Installing collected packages: install, pyaml, scikit-optimize\n",
      "Successfully installed install-1.3.4 pyaml-21.8.3 scikit-optimize-0.9.dev0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install install git+https://github.com/scikit-optimize/scikit-optimize.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "parameter = {\n",
    "    \"learning_rate\": Real(0.0001, 0.01, prior=\"log-uniform\"),\n",
    "    \"learning_rate_decay\": Real(0.0001, 0.01, prior=\"log-uniform\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_parameter(learning_rate=0.01, learning_rate_decay= 0.01): \n",
    "    # store function parameters (locals()) and local parameters defined so far\n",
    "    loc_params = dict(vars())\n",
    "    loc_params.update(params)\n",
    "        \n",
    "    return build_model(loc_params)\n",
    "\n",
    "\n",
    "def build_model(params):\n",
    "    \n",
    "    from fhnw.nlp.utils.params import compile_model\n",
    "    from fhnw.nlp.utils.params import build_model_cnn\n",
    "    from fhnw.nlp.utils.params import build_model_rnn\n",
    "        \n",
    "    model_type = params.get(\"model_type\", \"cnn\")\n",
    "    \n",
    "    if model_type == \"cnn\":\n",
    "        model = build_model_cnn(params)\n",
    "    elif model_type == \"rnn\":\n",
    "        model = build_model_rnn(params)\n",
    "    else:\n",
    "        raise TypeError(\"Unknown model_type \"+ model_type)\n",
    "        \n",
    "    compile_model(params, model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "scoring_function = make_scorer(f1_score, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "training_epochs = params.get(\"training_epochs\", 5)\n",
    "num_folds = params.get(\"cross_validation_num_folds\", 3)\n",
    "batch_size = params.get(\"batch_size\", 128)\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model_parameter, epochs=training_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    # (parameter space, # of evaluations)\n",
    "    [(parameter, 15)],\n",
    "    scoring=scoring_function,\n",
    "    #n_jobs=-1, \n",
    "    cv=StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_column_name = params.get(\"X_column_name\", \"text_clean\")\n",
    "y_column_name = params.get(\"y_column_name\", \"sentiment\")\n",
    "\n",
    "X_train = data_cross_validate[X_column_name]\n",
    "y_train = data_cross_validate[y_column_name]\n",
    "#X_train = data[X_column_name]\n",
    "#y_train = data[y_column_name]\n",
    "\n",
    "_ = opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skopt.plots import plot_objective\n",
    "\n",
    "_ = plot_objective(opt.optimizer_results_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update(opt.best_params_)\n",
    "\n",
    "model = build_model(params)\n",
    "model.summary()\n",
    "\n",
    "compile_model(params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"batch_size\"] = 128\n",
    "data_train, data_val = get_train_test_split(params, data_cross_validate)\n",
    "#data_train, data_val = get_train_test_split(params, data)\n",
    "\n",
    "dataset_train = dataframe_to_dataset(params, data_train)\n",
    "dataset_val = dataframe_to_dataset(params, data_val)\n",
    "\n",
    "params[\"training_epochs\"] = 10\n",
    "history = train_model(params, model, dataset_train, dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it is possible to further improve the model by unfreezing the embedding layer.\n",
    "\n",
    "Also see https://www.tensorflow.org/guide/keras/transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all layers (i.e. make embeddings trainable)\n",
    "model.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_compile_model(params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"training_epochs\"] = 2\n",
    "history = train_model(params, model, dataset_train, dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance on all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from fhnw.nlp.utils.ploting import report_classification_results\n",
    "\n",
    "report_classification_results(params, data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from fhnw.nlp.utils.ploting import report_classification_results\n",
    "\n",
    "report_classification_results(params, data_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "\n",
    "init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_dict(to_extend, other):\n",
    "    for key, value in other.items():\n",
    "        if key not in to_extend:\n",
    "            to_extend[key] = value\n",
    "            \n",
    "    return to_extend\n",
    "\n",
    "def get_model_checkpoint_path(params):\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    model_type = params.get(\"model_type\", \"unknown\")\n",
    "    \n",
    "    path = os.path.join(\".\", \"checkpoints\", model_type, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    params[\"model_path\"]=path\n",
    "    \n",
    "    return path\n",
    "\n",
    "def get_storable_params(params):\n",
    "    store_params = {}\n",
    "    tmp_params = {}\n",
    "    \n",
    "    for key, value in params.items():\n",
    "        tmp_params.clear()\n",
    "        tmp_params[key] = value\n",
    "        \n",
    "        try:\n",
    "            json_str = json.dumps(tmp_params)\n",
    "            if len(json_str) < 1000:\n",
    "                store_params[key] = value\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return store_params\n",
    "\n",
    "def store_params(params):\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    path = params[\"model_path\"]\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(path, \"params.json\"), \"w\") as fp:\n",
    "        json.dump(get_storable_params(params), fp, indent=2)\n",
    "\n",
    "def get_fold_splits(params, data):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    num_folds = params.get(\"cross_validation_num_folds\", 3)\n",
    "    X_column_name = params.get(\"X_column_name\", \"text_clean\")\n",
    "    y_column_name = params.get(\"y_column_name\", \"sentiment\")\n",
    "    computed_objects_column_name = params.get(\"computed_objects_column_name\", \"computed_objects\")\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    folds = []\n",
    "        \n",
    "    for train_index, val_index in kfold.split(data[X_column_name], data[y_column_name]):\n",
    "        fold = {}\n",
    "        fold[\"train_index\"] = train_index\n",
    "        fold[\"val_index\"] = val_index\n",
    "        folds.append(fold)\n",
    "        \n",
    "    params[computed_objects_column_name][\"cross_validation_fold_indices\"] = folds\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def evaluate_score(histories, eval_function_name):\n",
    "    epochs = len(histories[0].history.keys())\n",
    "    eval_last_n = min(max(int(0.3 * epochs), 3), epochs)\n",
    "    \n",
    "    # look at the last n epochs and get the mean and standard deviation of the validation score\n",
    "    last_n_scores = np.array([history.history[eval_function_name][-eval_last_n:] for history in histories])\n",
    "    mean = last_n_scores.mean()\n",
    "    std = last_n_scores.std()\n",
    "    \n",
    "    # if the model did not converge then set a high loss.\n",
    "    if np.isnan(mean):\n",
    "        mean = 9999.0\n",
    "        std = 0.0\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def build_model(params):    \n",
    "    from fhnw.nlp.utils.params import compile_model\n",
    "    from fhnw.nlp.utils.params import build_model_cnn\n",
    "    from fhnw.nlp.utils.params import build_model_rnn\n",
    "        \n",
    "    model_type = params.get(\"model_type\", \"cnn\")\n",
    "    \n",
    "    if model_type == \"cnn\":\n",
    "        model = build_model_cnn(params)\n",
    "    elif model_type == \"rnn\":\n",
    "        model = build_model_rnn(params)\n",
    "    else:\n",
    "        raise TypeError(\"Unknown model_type \"+ model_type)\n",
    "        \n",
    "    compile_model(params, model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cross_validate(params, data, fold_no, fold):\n",
    "    import os\n",
    "    \n",
    "    verbose = params.get(\"verbose\", False)\n",
    "    training_epochs = params.get(\"training_epochs\", 5)\n",
    "    checkpoint_path = get_model_checkpoint_path(params)\n",
    "    store_params(params)\n",
    "    \n",
    "    train_index = fold[\"train_index\"]\n",
    "    val_index = fold[\"val_index\"]\n",
    "    data_train = data.iloc[train_index]\n",
    "    data_val = data.iloc[val_index]\n",
    "    dataset_train = dataframe_to_dataset(params, data_train)\n",
    "    dataset_val = dataframe_to_dataset(params, data_val)\n",
    "        \n",
    "    model = build_model(params)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "            \n",
    "    callbacks = [\n",
    "        #tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_path, str(fold_no), \"model.{epoch:02d}\"), save_weights_only=False, monitor=\"val_loss\", mode=\"min\", save_best_only=True),\n",
    "        #tf.keras.callbacks.TensorBoard(log_dir=os.path.join(\".\", \"logs\")),\n",
    "    ]\n",
    "        \n",
    "    history = model.fit(dataset_train, validation_data=dataset_val, epochs=training_epochs, callbacks=callbacks)\n",
    "        \n",
    "    if verbose:\n",
    "        plot_history(history)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def do_cross_validation(params, data):\n",
    "    \n",
    "    #import psutil\n",
    "    #from multiprocess import Pool\n",
    "    #from functools import partial\n",
    "    \n",
    "    folds = get_fold_splits(params, data)\n",
    "    \n",
    "    #cv_fold_params = [(params, data, fold_no, fold) for fold_no, fold in enumerate(folds)]\n",
    "    #n_cores = psutil.cpu_count(logical=False)\n",
    "    #pool = Pool(n_cores)\n",
    "    #histories = pool.map(cross_validate, cv_fold_params)\n",
    "    #pool.close()\n",
    "    #pool.join()\n",
    "    \n",
    "    histories = []\n",
    "    for fold_no, fold in enumerate(folds):\n",
    "        history = cross_validate(params, data, fold_no, fold)\n",
    "        histories.append(history)\n",
    "    \n",
    "    return histories\n",
    "    \n",
    "\n",
    "def evaluate(params, data, eval_function_name):       \n",
    "    histories = do_cross_validation(params, data)\n",
    "    mean, std = evaluate_score(histories, eval_function_name)\n",
    "    \n",
    "    return {eval_function_name: (mean, std)}\n",
    "\n",
    "def hyperparameter_tuning(params, data):\n",
    "    eval_function_name = params.get(\"eval_function_name\", \"val_loss\")\n",
    "    eval_function_is_minimize = params.get(\"eval_function_is_minimize\", True)\n",
    "    hyper_parameters=[\n",
    "            {\n",
    "                \"name\": \"learning_rate\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [1e-5, 1e-1],\n",
    "                \"value_type\": \"float\",  \n",
    "                \"log_scale\": True, \n",
    "            },\n",
    "            {\n",
    "                \"name\": \"learning_rate_decay\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [1e-5, 1e-1],\n",
    "                \"value_type\": \"float\",  \n",
    "                \"log_scale\": True, \n",
    "            },\n",
    "        ]\n",
    "    hyperparameter_search_trials = params.get(\"hyperparameter_search_trials\", max(15, 5 * len(hyper_parameters)))\n",
    "    \n",
    "    ax_client = AxClient(enforce_sequential_optimization=False)\n",
    "    \n",
    "    ax_client.create_experiment(\n",
    "        name=\"hyperparameter_tuning\",\n",
    "        parameters=hyper_parameters,\n",
    "        objective_name=eval_function_name,\n",
    "        minimize=eval_function_is_minimize, \n",
    "    )\n",
    "    \n",
    "    for i in range(hyperparameter_search_trials):\n",
    "        parameters, trial_index = ax_client.get_next_trial()\n",
    "        parameters = extend_dict(parameters, params)\n",
    "        \n",
    "        ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters, data, eval_function_name))\n",
    "    \n",
    "    return ax_client\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ax_client = hyperparameter_tuning(params, data_cross_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"hp_tuning/german_doctor_reviews/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "ax_client.save_to_json_file(path+\"keras_run_01.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_trials_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, covariances = values\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.utils.notebook.plotting import render\n",
    "from ax.plot.contour import interact_contour\n",
    "\n",
    "eval_function_name = params.get(\"eval_function_name\", \"val_loss\")\n",
    "\n",
    "render(interact_contour(model=ax_client.generation_strategy.model, metric_name=eval_function_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
