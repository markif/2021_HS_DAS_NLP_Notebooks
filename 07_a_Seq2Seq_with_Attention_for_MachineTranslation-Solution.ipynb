{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# Machine Translation using Seq2Seq and Attention\n",
    "\n",
    "extended by Fabian Märki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to get an understanding of the sequence-to-sequence model with attention by applying it to machine translation.\n",
    "\n",
    "## Note\n",
    "Use your GPU (or run this notebook on google colab) for optimal performance.\n",
    "\n",
    "\n",
    "## Credits\n",
    "This notebook builds on a [notebook from tensorflow](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) (see below for copyrights).\n",
    "\n",
    "## Links\n",
    "- Detailed [seq2seq with attention notebook](https://www.tensorflow.org/text/tutorials/nmt_with_attention) on Machine Translation\n",
    "- Detailed [transformer notebook](https://www.tensorflow.org/text/tutorials/transformer) on Machine Translation\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/2021_HS_DAS_NLP_Notebooks/blob/master/07_a_Seq2Seq_with_Attention_for_MachineTranslation-Solution.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Steps\n",
    "\n",
    "Here are some tasks you could try:\n",
    "\n",
    "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
    "* Experiment with training on a larger dataset, or using more epochs.\n",
    "* Try a different RNN type (the original notebook uses LSTMs - since GRUs and RNNs do not have a cell state some adaptations to the original notebook were necessary)\n",
    "* Try a different attention type (e.g. LuongAttention or BahdanauAttention)\n",
    "* Think about further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fhnw-nlp-utils>=0.1.6\n",
      "  Downloading fhnw_nlp_utils-0.1.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.1.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.3.4)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.13.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (0.24.2)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.2)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (0.70.12.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (1.8.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (3.6.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.6) (5.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.6) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.6) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (8.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.6) (0.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (4.62.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (3.0.12)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.6) (2.26.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.6) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.6) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.6) (1.0.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.6/dist-packages (from multiprocess->fhnw-nlp-utils>=0.1.6) (0.3.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.6) (2021.8.28)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.6) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.6) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.6) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.6) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.6) (2.0.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from click->nltk->fhnw-nlp-utils>=0.1.6) (4.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.6) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.6) (3.7.4.3)\n",
      "Installing collected packages: fhnw-nlp-utils, PySocks\n",
      "  Attempting uninstall: fhnw-nlp-utils\n",
      "    Found existing installation: fhnw-nlp-utils 0.1.3\n",
      "    Uninstalling fhnw-nlp-utils-0.1.3:\n",
      "      Successfully uninstalled fhnw-nlp-utils-0.1.3\n",
      "Successfully installed PySocks-1.7.1 fhnw-nlp-utils-0.1.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Tensorflow version: 2.5.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "!pip install 'fhnw-nlp-utils>=0.1.6'\n",
    "\n",
    "from fhnw.nlp.utils.storage import download\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.\n",
      "Need to get 316 kB of archives.\n",
      "After this operation, 954 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 wget amd64 1.19.4-1ubuntu2.2 [316 kB]\n",
      "Fetched 316 kB in 0s (1502 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package wget.\n",
      "(Reading database ... 17369 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.19.4-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking wget (1.19.4-1ubuntu2.2) ...\n",
      "Setting up wget (1.19.4-1ubuntu2.2) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aElYAKlV2Mi"
   },
   "source": [
    "##### Copyright by the TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "wmYJlt6LWVOU"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n0dcDw1Wszw"
   },
   "source": [
    "## Overview\n",
    "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
    "In this noteboook you broadly cover four essential topics necessary for Neural Machine Translation:\n",
    "\n",
    "\n",
    "* **Data cleaning**\n",
    "* **Data preparation**\n",
    "* **Neural Translation Model with Attention**\n",
    "* **Final Translation with ```tf.addons.seq2seq.BasicDecoder``` and ```tf.addons.seq2seq.BeamSearchDecoder```** \n",
    "\n",
    "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. You focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpySVYWJhxaV"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_kxfdP4hJUPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons==0.11.2\n",
      "  Downloading tensorflow_addons-0.11.2-cp36-cp36m-manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.11.2 typeguard-2.12.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.5.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_vg-XNXTil"
   },
   "source": [
    "## Data Cleaning and Data Preparation \n",
    "\n",
    "You'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "---\n",
    "      May I borrow this book?    ¿Puedo tomar prestado este libro?\n",
    "---\n",
    "\n",
    "\n",
    "There are a variety of languages available, but you'll use the English-Spanish dataset. After downloading the dataset, here are the steps you'll take to prepare the data:\n",
    "\n",
    "\n",
    "1. Add a start and end token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a Vocabulary with word index (mapping from word → id) and reverse word index (mapping from id → word).\n",
    "5. Pad each sentence to a maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PvRnGWnvXm6l"
   },
   "outputs": [],
   "source": [
    "from fhnw.nlp.utils.storage import download           \n",
    "\n",
    "def download_nmt(problem_type):\n",
    "    import os\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    \n",
    "    frm = problem_type.split(\"-\")[0]\n",
    "    zip_file = problem_type+\".zip\"\n",
    "    path_to_zip = \"data\"\n",
    "    path_to_zip_file = os.path.join(path_to_zip, zip_file)\n",
    "\n",
    "    download(\"http://www.manythings.org/anki/\"+zip_file, path_to_zip_file)\n",
    "\n",
    "    shutil.unpack_archive(path_to_zip_file, path_to_zip)\n",
    "\n",
    "    path_to_file = pathlib.Path(path_to_zip)/str(frm+\".txt\")\n",
    "\n",
    "    # cleanup\n",
    "    os.remove(os.path.join(path_to_zip, \"_about.txt\"))\n",
    "    \n",
    "    return path_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFKB2c_tX4wU"
   },
   "source": [
    "### Define a NMTDataset class with necessary functions to follow Step 1 to Step 4. \n",
    "The ```call()``` will return:\n",
    "1. ```train_dataset```  and ```val_dataset``` : ```tf.data.Dataset``` objects\n",
    "2. ```inp_lang_tokenizer``` and ```targ_lang_tokenizer``` : ```tf.keras.preprocessing.text.Tokenizer``` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JMAHz7kJXc5N"
   },
   "outputs": [],
   "source": [
    "class NMTDataset:\n",
    "    def __init__(self, problem_type):\n",
    "        self.problem_type = problem_type\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "    \n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    ## Step 1 and Step 2 \n",
    "    def preprocess_sentence(self, w):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "        w = w.strip()\n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    \n",
    "    def create_dataset(self, path, num_examples):\n",
    "        # path : path to spa-eng.txt file\n",
    "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')[0:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "        return zip(*word_pairs)\n",
    "\n",
    "    # Step 3 and Step 4\n",
    "    def tokenize(self, lang):\n",
    "        # lang = list of sentences in a language\n",
    "        \n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        # creating cleaned input, output pairs\n",
    "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
    "\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
    "        file_path = download_nmt(self.problem_type)\n",
    "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
    "        \n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "<font color='blue'>Switch to a different language by setting **translation_type** accordingly</font> (see [here](http://www.manythings.org/anki/) for more options).\n",
    "\n",
    "\n",
    "<font color='blue'>Set **num_examples** to **None** to train a model with all training translation pairs.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EIW4NVBmJ25k"
   },
   "outputs": [],
   "source": [
    "translation_type = \"spa-eng\"\n",
    "#translation_type = \"deu-eng\"\n",
    "\n",
    "# Let's limit #training examples for faster training\n",
    "num_examples = 30000\n",
    "#num_examples = None\n",
    "\n",
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset_creator = NMTDataset(translation_type)\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w2lCTy4vKOkB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 19]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "Some more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "g-yY9c6aIu1h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length: spa-eng  and vocab_size: spa-eng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((19, 11), (9323, 4795))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length:\", translation_type, \" and vocab_size:\",  translation_type)\n",
    "(max_length_input, max_length_output), (vocab_inp_size, vocab_tar_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "<font color='blue'>Try a different RNN type like **GRU** or **RNN**. **NOTE:** This required some adaptations (in comparison to the original notebook) since GRU and RNN do not have a cell state (unlike LSTMs).</font>\n",
    "\n",
    "\n",
    "<font color='red'>In the last lecture, we discussed two RNN types which help to mitigate the vanishing gradient problem. What is your intuition about the effect of using the one vs. the other?</font>\n",
    "\n",
    "<font color='green'>In the lecture we discussed the LSTM and the GRU. The initial notebook uses LSTM for the Encoder and the Decoder. </font>\n",
    "\n",
    "- GRUs are simpler, take less time to train and execute faster\n",
    "- GRUs perform better on less training data\n",
    "- LSTMs are able to remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations\n",
    "- Vanilla RNNs are probably too simplistic to capture the full complexity of the MT task\n",
    "\n",
    "[This](https://arxiv.org/pdf/1412.3555v1.pdf) and [this](https://arxiv.org/pdf/1702.01923.pdf) could be of further interest.</font>\n",
    "\n",
    "<font color='red'>What further improvements could be applied to this model?</font>\n",
    "\n",
    "<font color='green'>Pre-trained Embeddings could help to further improve the translations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "##### \n",
    "rnn_type = \"lstm\"\n",
    "#rnn_type = \"gru\"\n",
    "#rnn_type = \"rnn\"\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    \n",
    "    if rnn_type == \"lstm\":\n",
    "        self.rnn_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    elif rnn_type == \"gru\":\n",
    "        self.rnn_layer = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    elif rnn_type == \"rnn\":\n",
    "        self.rnn_layer = tf.keras.layers.SimpleRNN(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        raise TypeError(\"Unknown rnn_type \"+rnn_type)\n",
    "    \n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    if rnn_type == \"lstm\":\n",
    "        output, h, c = self.rnn_layer(x, initial_state = hidden)\n",
    "        return output, [h, c]\n",
    "    else:\n",
    "        output, h = self.rnn_layer(x, initial_state = hidden)\n",
    "        return output, h\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    if rnn_type == \"lstm\":\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n",
    "    else:\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "  def initialize_start_state(self, batch_size, units):\n",
    "    if rnn_type == \"lstm\":\n",
    "        return [tf.zeros((batch_size, units)), tf.zeros((batch_size, units))]\n",
    "    else:\n",
    "        return tf.zeros((batch_size, units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 19, 1024)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_states = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.attention_type = attention_type\n",
    "    \n",
    "    # Embedding Layer\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    #Final Dense layer on which softmax will be applied\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "    if rnn_type == \"lstm\":\n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "    elif rnn_type == \"gru\":\n",
    "        self.decoder_rnn_cell = tf.keras.layers.GRUCell(self.dec_units)\n",
    "    elif rnn_type == \"rnn\":\n",
    "        self.decoder_rnn_cell = tf.keras.layers.SimpleRNNCell(self.dec_units)\n",
    "    else:\n",
    "        raise TypeError(\"Unknown rnn_type \"+rnn_type)\n",
    "\n",
    "\n",
    "    # Sampler\n",
    "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "    \n",
    "  def build_rnn_cell(self, batch_sz):\n",
    "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units, alignment_history=True)\n",
    "    return rnn_cell\n",
    "\n",
    "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "    if(attention_type=='bahdanau'):\n",
    "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "    else:\n",
    "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)   \n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "    return decoder_initial_state\n",
    "\n",
    "  def call(self, inputs, initial_state):\n",
    "    x = self.embedding(inputs)\n",
    "    outputs, output_states, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "<font color='blue'>Try different attention types by modifying **attention_type**.</font>\n",
    "\n",
    "<font color='red'>This implementation uses Bahdanau's additive attention score. What is your intuition about the effect when it would be replaced by the dot-product score (luong)?</font>\n",
    "\n",
    "<font color='green'>Additive attention uses a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "    \n",
    "See also [Attention Is All You Need, section 3.2.1](https://arxiv.org/pdf/1706.03762.pdf)</font>\n",
    "\n",
    "\n",
    "<font color='red'>Can you think of options to further improve the dot-product score (luong)?</font>\n",
    "\n",
    "<font color='green'>One could use the scaled dot-product score. The motivation for scaling is to improve learning and thus training could converge faster. BTW: tfa.seq2seq.LuongAttention provides a **scale** parameter.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DaiO0Z6_Ml1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (64, 10, 4795)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "attention_type = \"bahdanau\"\n",
    "#attention_type = \"luong\"\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention_type)\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, sample_states, tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  # real shape = (BATCH_SIZE, max_length_output)\n",
    "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "  loss = mask* loss\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  return loss  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bw95utNiFHa"
   },
   "source": [
    "## One train_step operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_state = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "    real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "    # Set the AttentionMechanism object with encoder_outputs\n",
    "    decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, enc_state, tf.float32)\n",
    "    pred = decoder(dec_input, decoder_initial_state)\n",
    "    logits = pred.rnn_output\n",
    "    loss = loss_function(real, logits)\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pey8eb9piMMg"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9783\n",
      "Epoch 1 Batch 100 Loss 2.1086\n",
      "Epoch 1 Batch 200 Loss 1.9023\n",
      "Epoch 1 Batch 300 Loss 1.6813\n",
      "Epoch 1 Loss 1.6490\n",
      "Time taken for 1 epoch 30.49012780189514 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4602\n",
      "Epoch 2 Batch 100 Loss 1.3706\n",
      "Epoch 2 Batch 200 Loss 1.4317\n",
      "Epoch 2 Batch 300 Loss 1.3317\n",
      "Epoch 2 Loss 1.0907\n",
      "Time taken for 1 epoch 22.078705072402954 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1225\n",
      "Epoch 3 Batch 100 Loss 0.9918\n",
      "Epoch 3 Batch 200 Loss 0.9152\n",
      "Epoch 3 Batch 300 Loss 0.8204\n",
      "Epoch 3 Loss 0.7684\n",
      "Time taken for 1 epoch 21.459174156188965 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6987\n",
      "Epoch 4 Batch 100 Loss 0.6949\n",
      "Epoch 4 Batch 200 Loss 0.6061\n",
      "Epoch 4 Batch 300 Loss 0.6431\n",
      "Epoch 4 Loss 0.5098\n",
      "Time taken for 1 epoch 21.920538425445557 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3735\n",
      "Epoch 5 Batch 100 Loss 0.4552\n",
      "Epoch 5 Batch 200 Loss 0.4212\n",
      "Epoch 5 Batch 300 Loss 0.3476\n",
      "Epoch 5 Loss 0.3489\n",
      "Time taken for 1 epoch 21.629646062850952 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2639\n",
      "Epoch 6 Batch 100 Loss 0.3505\n",
      "Epoch 6 Batch 200 Loss 0.3722\n",
      "Epoch 6 Batch 300 Loss 0.3269\n",
      "Epoch 6 Loss 0.2517\n",
      "Time taken for 1 epoch 22.17528533935547 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.2174\n",
      "Epoch 7 Batch 100 Loss 0.1942\n",
      "Epoch 7 Batch 200 Loss 0.2105\n",
      "Epoch 7 Batch 300 Loss 0.2148\n",
      "Epoch 7 Loss 0.1935\n",
      "Time taken for 1 epoch 21.588039875030518 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1488\n",
      "Epoch 8 Batch 100 Loss 0.2142\n",
      "Epoch 8 Batch 200 Loss 0.1651\n",
      "Epoch 8 Batch 300 Loss 0.2297\n",
      "Epoch 8 Loss 0.1622\n",
      "Time taken for 1 epoch 22.01940155029297 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1368\n",
      "Epoch 9 Batch 100 Loss 0.1287\n",
      "Epoch 9 Batch 200 Loss 0.1838\n",
      "Epoch 9 Batch 300 Loss 0.1640\n",
      "Epoch 9 Loss 0.1390\n",
      "Time taken for 1 epoch 21.429432153701782 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1496\n",
      "Epoch 10 Batch 100 Loss 0.1518\n",
      "Epoch 10 Batch 200 Loss 0.1484\n",
      "Epoch 10 Batch 300 Loss 0.2156\n",
      "Epoch 10 Loss 0.1244\n",
      "Time taken for 1 epoch 22.036405086517334 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Use tf-addons BasicDecoder for decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = encoder.initialize_start_state(inference_batch_size, units)\n",
    "  enc_out, enc_state = encoder(inputs, enc_start_state)\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "  # Instantiate BasicDecoder object\n",
    "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "  # Setup Memory in decoder stack\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "  # set decoder_initial_state\n",
    "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, enc_state, tf.float32)\n",
    "\n",
    "\n",
    "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "  \n",
    "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "  \n",
    "  return outputs.sample_id.numpy()\n",
    "\n",
    "def translate(sentence):\n",
    "  result = evaluate_sentence(sentence)\n",
    "  print(result)\n",
    "  result = targ_lang.sequences_to_texts(result)\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f97f48236a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WYmYhNN_faR5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  12  52 181  40   4   3]]\n",
      "Input: hace mucho frio aqui.\n",
      "Predicted translation: ['it s very cold here . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20   9  23 207   4   3]]\n",
      "Input: esta es mi vida.\n",
      "Predicted translation: ['this is my life . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  6 42 93  8  3]]\n",
      "Input: ¿todavia estan en casa?\n",
      "Predicted translation: ['are you on home ? <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 86  16 217  73   4   3]]\n",
      "Input: trata de averiguarlo.\n",
      "Predicted translation: ['try to find out . <end>']\n"
     ]
    }
   ],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRUuNDeY0HiC"
   },
   "source": [
    "## Use tf-addons BeamSearchDecoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AJ-RTQ0hsJNL"
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  inference_batch_size = inputs.shape[0]\n",
    "  result = ''\n",
    "\n",
    "  enc_start_state = encoder.initialize_start_state(inference_batch_size, units)\n",
    "  enc_out, enc_state = encoder(inputs, enc_start_state)\n",
    "\n",
    "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "  end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  # From official documentation\n",
    "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "  decoder.attention_mechanism.setup_memory(enc_out)\n",
    "  print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "  hidden_state = tfa.seq2seq.tile_batch(enc_state, multiplier=beam_width)\n",
    "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "  # Instantiate BeamSearchDecoder\n",
    "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "  # The final beam predictions are stored in outputs.predicted_id\n",
    "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "  \n",
    "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "  \n",
    "  return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "g_LvXGvX8X-O"
   },
   "outputs": [],
   "source": [
    "def beam_translate(sentence, beam_width):\n",
    "  result, beam_scores = beam_evaluate_sentence(sentence, beam_width)\n",
    "  print(result.shape, beam_scores.shape)\n",
    "  for beam, score in zip(result, beam_scores):\n",
    "    print(beam.shape, score.shape)\n",
    "    output = targ_lang.sequences_to_texts(beam)\n",
    "    output = [a[:a.index('<end>')] for a in output]\n",
    "    beam_score = [a.sum() for a in score]\n",
    "    print('Input: %s' % (sentence))\n",
    "    for i in range(len(output)):\n",
    "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "<font color='blue'>Try different values for **beam_width**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TODnXBleDzzO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 19, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/resource_loader.py:103: UserWarning: You are currently using TensorFlow 2.5.1 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.2.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.2.0 and strictly below 2.3.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  UserWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/options.py:47: RuntimeWarning: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/beam_search_decoder.py\", line 239, in gather_tree\n",
      "    return _beam_search_so.ops.addons_gather_tree(\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/resource_loader.py\", line 64, in ops\n",
      "    self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py\", line 58, in load_op_library\n",
      "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow_addons/custom_ops/seq2seq/_beam_search_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n",
      "\n",
      "\n",
      "The gather_tree C++/CUDA custom op could not be loaded.\n",
      "For this reason, Addons will fallback to an implementation written\n",
      "in Python with public TensorFlow ops. There worst you might experience with\n",
      "this is a moderate slowdown on GPU. There can be multiple\n",
      "reason for this loading error, one of them may be an ABI incompatibility between\n",
      "the TensorFlow installed on your system and the TensorFlow used to compile\n",
      "TensorFlow Addons' custom ops. The stacktrace generated when loading the\n",
      "shared object file was displayed above.\n",
      "\n",
      "If you want this warning to disappear, either make sure the TensorFlow installed\n",
      "is compatible with this version of Addons, or tell TensorFlow Addons to\n",
      "prefer using Python implementations and not custom C++/CUDA ones. You can do that\n",
      "by changing the TF_ADDONS_PY_OPS flag\n",
      "either with the environment variable:\n",
      "```bash\n",
      "TF_ADDONS_PY_OPS=1 python my_script.py\n",
      "```\n",
      "or in your code, after your imports:\n",
      "```python\n",
      "import tensorflow_addons as tfa\n",
      "import ...\n",
      "import ...\n",
      "\n",
      "tfa.options.TF_ADDONS_PY_OPS = True\n",
      "```\n",
      "\n",
      "  warnings.warn(warning_msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "(1, 3, 7) (1, 3, 7)\n",
      "(3, 7) (3, 7)\n",
      "Input: hace mucho frio aqui.\n",
      "1 Predicted translation: it s very cold here .   -1.5497076511383057\n",
      "2 Predicted translation: there s very cold here .   -19.6527099609375\n",
      "3 Predicted translation: that s very cold here .   -22.717140197753906\n"
     ]
    }
   ],
   "source": [
    "beam_width = 3\n",
    "\n",
    "beam_translate(u'hace mucho frio aqui.', beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_BezQwENFY3L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 19, 1024)\n",
      "(1, 3, 6) (1, 3, 6)\n",
      "(3, 6) (3, 6)\n",
      "Input: ¿todavia estan en casa?\n",
      "1 Predicted translation: aren t you home ?   -5.952308177947998\n",
      "2 Predicted translation: are you on home ?   -8.207381248474121\n",
      "3 Predicted translation: are you home ?   -15.489701271057129\n"
     ]
    }
   ],
   "source": [
    "beam_translate(u'¿todavia estan en casa?', beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "networks_seq2seq_nmt.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
